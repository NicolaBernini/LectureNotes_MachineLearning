
# Overview 

The Training of Deep Neural Networks (DNN) can be performed with the Backpropagation Algo just as for Shallow Networks but specific issues could arise 



# Issues 

## Exploding and Vanishing Gradient 

The Gradient of the Loss Function depends on the Neurons Transfer Function (NTF) definition 

The presence of "Non Linear Saturating Transfer Function" (NLSTF) could produce very big or very little gradient values at training time, producing the so called phenomena of Exploding and Vanishing Gradient respectively. 

The probability of this phenomenon grows with the Network Depth hence it is a common issue in the Deep Learning context 

Different Solutions could be used 











