{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Basics1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "TGhjtnudq8cH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vkYK4OXOq9TK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Function Approximator \n",
        "\n",
        "Let's consider $ f \\in \\mathcal{F} $ a Function Approximator so that \n",
        "\n",
        "$$ f : \\mathcal{X} \\rightarrow \\mathcal{Y} $$\n",
        "\n",
        "Let's consider $ f_{\\theta} $ a Parametric Function Approximator with $ \\theta $ Params so that \n",
        "\n",
        "$$ f : \\mathcal{X} \\times \\mathcal{\\Theta} \\rightarrow \\mathcal{Y} $$\n",
        "\n",
        "Let's denote with $ \\hat y \\in \\mathcal{Y} $ its estimation for a given $ x \\in \\mathcal{X} $ Input assuming a certain $ \\theta \\in \\Theta $ Params Set\n",
        "\n",
        "$$ \\hat y = f_{\\theta}(x) = f(x; \\theta) $$\n",
        "\n",
        "\n",
        "# Metrics \n",
        "\n",
        "## Loss Function \n",
        "\n",
        "Let's consider a $ f \\in \\mathcal{F} $ Function Approximator which is able to perform predictions such as $ \\hat y = f(x) $ for some $ x \\in \\mathcal{X}, \\hat y \\in \\mathcal{Y} $ \n",
        "\n",
        "To get a quantitative measure of how well the $ f $ predictor is working, let's introduce the Loss Function $ L \\in \\mathcal{L} $ as a dissimilarity measure of its 2 arguments \n",
        "\n",
        "$$ L : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}^{+} $$\n",
        "\n",
        "\n",
        "## Risk \n",
        "\n",
        "The Risk definition involves a certain PDF $ D $ where it is possible to draw $ (x,y) $ Pairs \n",
        "\n",
        "$$ R(f, P) = E_{(x,y) \\sim P} \\left [ L(f(x), y) \\right ] $$\n",
        "\n",
        "\n",
        "## Empirical Risk \n",
        "\n",
        "The Empirical Risk typically regards a certain Sampled PDF $ D^{(ds)} = \\{ (x,y)_{i} \\}_{i=1,...,N} $ called Dataset \n",
        "\n",
        "### Notes \n",
        "\n",
        "- Typically the Dataset is split into Training Set and Validation Set : the former is used to perform the Training while the second is used to check the Generalization \n",
        "\n",
        "$$ D^{(ds)} = D^{(ts)} \\cup D^{(vs)} $$\n",
        "\n",
        "- In Stochastic Gradient Descent based Training the Training Set gets sub-sampled into Batches so that the Training actually happens according to the $ D_{t}^{(b)} $ Batch for a specific $ t $ Training Iteration \n",
        "\n",
        "$$ D_{t}^{(b)} \\subset D^{(ts)} $$\n",
        "\n"
      ]
    }
  ]
}
