{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Basics1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "TGhjtnudq8cH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vkYK4OXOq9TK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Function Approximator \n",
        "\n",
        "Let's consider $ f \\in \\mathcal{F} $ a Function Approximator so that \n",
        "\n",
        "$$ f : \\mathcal{X} \\rightarrow \\mathcal{Y} $$\n",
        "\n",
        "Let's consider $ f_{\\theta} $ a Parametric Function Approximator with $ \\theta $ Params so that \n",
        "\n",
        "$$ f : \\mathcal{X} \\times \\mathcal{\\Theta} \\rightarrow \\mathcal{Y} $$\n",
        "\n",
        "Let's denote with $ \\hat y \\in \\mathcal{Y} $ its estimation for a given $ x \\in \\mathcal{X} $ Input assuming a certain $ \\theta \\in \\Theta $ Params Set\n",
        "\n",
        "$$ \\hat y = f_{\\theta}(x) = f(x; \\theta) $$\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "aCtmA26R2Ns8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Metrics \n",
        "\n",
        "## Loss Function \n",
        "\n",
        "Let's consider a $ f \\in \\mathcal{F} $ Function Approximator which is able to perform predictions such as $ \\hat y = f(x) $ for some $ x \\in \\mathcal{X}, \\hat y \\in \\mathcal{Y} $ \n",
        "\n",
        "To get a quantitative measure of how well the $ f $ predictor is working, let's introduce the Loss Function $ L \\in \\mathcal{L} $ as a dissimilarity measure of its 2 arguments \n",
        "\n",
        "$$ L : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}^{+} $$\n",
        "\n",
        "\n",
        "### Examples \n",
        "\n",
        "#### Binary Loss Function \n",
        "\n",
        "The $ L^{(bin)}(a,b) $ returns $ 0 $ only when $ a = b $ and $ 1 $ otherwise \n",
        "\n",
        "$$ L^{(bin)}(a,b) = \\left\\{\\begin{matrix}\n",
        "0 \\qquad a = b\n",
        "\\\\ \n",
        "1 \\qquad a \\neq b\n",
        "\\end{matrix}\\right. $$\n",
        "\n",
        "\n",
        "#### Absolute Loss Function \n",
        "\n",
        "The $ L^{(abs)}(a,b) $ returns the Absolute Difference \n",
        "\n",
        "$$ L^{(abs)}(a,b) = | a - b | $$\n",
        "\n",
        "\n",
        "#### Square Loss Function \n",
        "\n",
        "The $ L^{(sq)}(a,b) $ returns the Square Difference \n",
        "\n",
        "$$ L^{(sq)} = (x - y)^{2} $$\n",
        "\n",
        "\n",
        "## Risk \n",
        "\n",
        "The Risk definition involves a certain PDF $ D $ where it is possible to draw $ (x,y) $ Pairs \n",
        "\n",
        "$$ R(f, P) = E_{(x,y) \\sim P} \\left [ L(f(x), y) \\right ] $$\n",
        "\n",
        "\n",
        "## Empirical Risk \n",
        "\n",
        "The Empirical Risk typically involves a Sampled PDF Approximation $ D^{(ds)} = \\{ (x,y)_{i} \\}_{i=1,...,N} $ called Dataset so the Empirical Risk definition becomes \n",
        "\n",
        "$$ R(f, D^{(ds)}) = E_{(x,y) \\sim D^{(ds)}} \\left [ L(f(x), y) \\right ] $$\n",
        "\n",
        "\n",
        "### Expectation as Averaging \n",
        "\n",
        "If the $ E_{D^{(ds)}} \\left [ \\cdot \\right ] $ Expectation Operator is defined as an Average then the Empirical Risk becomes \n",
        "\n",
        "$$ R(f, D^{(ds)}) = \\frac{1}{N} \\sum_{i=1}^{N} L(f(x_{i}), y_{i}) $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Notes \n",
        "\n",
        "- Typically the Dataset is split into Training Set and Validation Set : the former is used to perform the Training while the second is used to check the Generalization \n",
        "\n",
        "$$ D^{(ds)} = D^{(ts)} \\cup D^{(vs)} $$\n",
        "\n",
        "- In Stochastic Gradient Descent based Training the Training Set gets sub-sampled into Batches so that the Training actually happens according to the $ D_{t}^{(b)} $ Batch for a specific $ t $ Training Iteration \n",
        "\n",
        "$$ D_{t}^{(b)} \\subset D^{(ts)} $$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LZVXhXf02OqL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Training \n",
        "\n",
        "## Standard Risk based Training \n",
        "\n",
        "In general the Training Problem is essentially a Search Problem in the $ \\Theta $ Parameters Space which is modeled in the Optimization Framework relying on a Risk Measure depending on a certain $ P $ Data Distribution\n",
        "\n",
        "$$ f^{\\star} = \\arg\\min_{\\theta \\in \\Theta} R(f_{\\theta}, P) $$\n",
        "\n",
        "## Empirical Risk based Training \n",
        "\n",
        "Practically the $ P $ True Data Distribution is unknown, instead the $ D^{(ds)} $ Dataset is available as a Sampled Approximation of $ P $ hence the Optimization Problem becomes \n",
        "\n",
        "$$ f^{\\dagger} = \\arg\\min_{\\theta \\in \\Theta} R(f_{\\theta}, D^{(ds)}) = \\arg\\min_{\\theta \\in \\Theta} \\frac{1}{N} \\sum_{i=1}^{N} L(f(x_{i}), y_{i}) $$\n",
        "\n",
        "Under regularity conditions there are theoretical convergence guarantess so that \n",
        "\n",
        "$$ f^{\\dagger} \\rightarrow f^{\\star} $$\n",
        "\n",
        "\n",
        "## Overfitting \n",
        "\n",
        "The $ P \\neq D^{(ds)} $ difference between the True Data PDF and the Sampled Data PDF used for the Training, hence composing the Training and Validation Set, can be responsible for the Model Overfitting along with a high model learning capacity, as in the case for Big Deep Neural Networks \n",
        "\n",
        "### Dataset Improvement \n",
        "\n",
        "In order to combat overfitting, the Big Data approach has led to the development of $ D^{(ds)} $ which are better approximator for $ P $ but especially in some applications the margin to improve is still big \n",
        "\n",
        "Beside increasing the Dataset Size with more elements, other approaches exist like Dataset Augmentation \n",
        "\n",
        "### Regularization \n",
        "\n",
        "The \"Regularization Path\" Mol consists of introducing a \"Prior Solution Measure Path\" Mol to the \"Search Path\" Mol which should work as a \"Compass\" Mol along the \"Minimization Path\" Mol : it should steer away from certain kind of solutions privileging other kinds \n",
        "\n",
        "Another approach to combact overfitting, regards adding $ \\rho( \\theta_{t} ) $ as a quality measure on a specific $ \\theta_{t} $ solution at $ t $ Training Time \n",
        "\n",
        "$$ \\rho : \\Theta \\rightarrow \\mathbb{R}^{+} $$\n",
        "\n",
        "This term can be added to the Risk Minimization Term to build a more complex Objective Function in the Optimization Problem to privilege e.g. simpler solutions like \n",
        "\n",
        "$$ f^{\\dagger} = \\arg\\min_{\\theta \\in \\Theta} \\left ( R(f, D^{(ds)}) + c \\rho(\\theta)  \\right )  $$\n",
        "\n",
        "\n",
        "#### Ridge Regularization \n",
        "\n",
        "The Ridge Regularization is defined as \n",
        "\n",
        "$$ \\rho^{(ridge)}(\\theta) = \\left \\| \\theta \\right \\|  $$\n",
        "\n",
        "hence it penalizes solutions with large weights, so implicitely making the $ P(\\theta) $ Distribution of $ \\theta $ more flattened (spikes are made unlikely) \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "k2Llt1TrHoVr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Training Methods \n",
        "\n",
        "The Params Fitting, also called Training in Machine Learning jargon, is typically solved using Iterative Numerical Estimation Methods so that \n",
        "\n",
        "$$ \\theta_{t+1} = s^{(tr)}(\\theta_{t}) $$\n",
        "\n",
        "is used to build $ S = \\{ \\theta_{i} \\}_{i=1,...,N_{s}} $ Solutions Sequence which can have different properties like \n",
        "\n",
        "- Convergence: related to its capability to converge to $ \\theta^{\\star} $ Optimum or at least to a $ \\theta^{\\dagger} $ Stationary Point $ \\nabla f(\\theta^{\\dagger}) = 0 $ \n",
        "- Convergence Rate: relatdee to its capability to converge according to a certain number of iterations \n",
        "\n",
        "\n"
      ]
    }
  ]
}
