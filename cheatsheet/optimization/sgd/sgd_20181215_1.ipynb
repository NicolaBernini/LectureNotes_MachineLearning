{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cheatsheet - Stochastic Gradient Descent1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "6vhGeMNgnhFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# SGD Overview \n",
        "\n",
        "## Setup \n",
        "\n",
        "- Typical objective function is of the form \n",
        "\n",
        "$$ f(x;\\theta) = E[f(X; \\theta)] $$\n",
        "\n",
        "with \n",
        "\n",
        "- the $f(\\cdot; \\cdot)$ model representing input and parametrization \n",
        "- the $X$ Random Variable representing the Input \n",
        "- the $\\theta$ Parametrization \n",
        "\n",
        "- Expected Value as Averaging over $\\{x_{i}\\}_{i=1,...,n}$ a set of input \n",
        "\n",
        "$$ \\hat \\theta = \\min_{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^{n} f(x_{i}; \\theta) $$\n",
        "\n",
        "\n",
        "## Notation \n",
        "\n",
        "- Indexes (unless otherwise stated)\n",
        "  - the $i$ counts over the Training Set items so \n",
        "    - $T = \\{x_{i}\\}_{i=1,...,n}$ represents the full training set and \n",
        "    - $\\tilde T = \\{x_{i}\\}_{i=1,...,\\tilde n < n}$ represents a random subset of the full Training Set \n",
        "  - the $j$ counts over the $N(\\Theta)$ Params Space Dimensions so \n",
        "    - $\\theta = \\{\\theta_{j}\\}_{j=1,...,N(\\Theta)}$ a certain parametrization is essentially an element in a vectorial space which can be represented with its coordinates \n",
        "\n",
        "Let's make the notation lighter \n",
        "\n",
        "- Expectation computed over the full Training Set \n",
        "\n",
        "$$ f(x;\\theta) = E[f(X; \\theta)] \\rightarrow f^{(T)}(\\theta) $$\n",
        "\n",
        "- It leads to **True Gradient** \n",
        "\n",
        "$$ \\nabla f^{(T)}(\\theta) $$\n",
        "\n",
        "\n",
        "\n",
        "## Stochastic Gradient \n",
        "\n",
        "- Let's just switch $T \\rightarrow \\tilde T$ hence passing from the Full Training Set to a Random Subset of this one hence we get \n",
        "\n",
        "$$ f^{(T)}(\\theta) \\rightarrow f^{(\\tilde T)}(\\theta) $$ \n",
        "\n",
        "- so in this case the expectation is computed not on the full Training Set but on the Subset \n",
        "\n",
        "- Finally the Full Gradient gets \n",
        "\n",
        "$$ \\nabla f^{(T)} \\rightarrow \\nabla f^{(\\tilde T)} $$\n",
        "\n",
        "- to the Approximated Gradient also called Stochastic Gradient \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OWxlDoo7sFFA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# True Gradient vs Stochastic Gradient Comparison \n",
        "\n",
        "- The True Gradient shows the Optimal Direction but being an overage on a lot of items \n",
        "  - it has a higher computational complexity \n",
        "    - it is $O(N)$ with the full Training Set elements \n",
        "  - signal can be very low (for internal cancellations + division by a big $n$) and step size choice is critical \n",
        "- The Stochastic Gradient shows a Suboptimal Direction but \n",
        "  - it has a lower computational complexity \n",
        "    - as $\\tilde n < n$\n",
        "  - signal can be stronger \n",
        "  \n",
        "- Even if the direction is suboptimal, that might not be so important as what it is fundamental is the global convergence property \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uGe5Lkt_ndL7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
